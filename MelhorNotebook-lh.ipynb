{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhslab/tech-challenge-fase4-lh/blob/main/MelhorNotebook-lh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wJjMKvNePgK"
      },
      "outputs": [],
      "source": [
        "!pip install deepface insightface tqdm onnxruntime mediapipe scikit-learn numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-Ov66-3Deoxa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from deepface import DeepFace\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "from insightface.app import FaceAnalysis\n",
        "import uuid\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mediapipe.python.solutions.hands import Hands\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "metadata": {
        "id": "E5xYtLbpSlhG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K6HQv_4qetWB"
      },
      "outputs": [],
      "source": [
        "def generate_summary(emotion_counts, total_frames, output_path):\n",
        "    # Exibir Relatório no Console\n",
        "    print(\"RELATÓRIO:\")\n",
        "\n",
        "    # Exibir o total de frames analisados\n",
        "    print(f\"\\nTotal de frames analisados: {total_frames}\")\n",
        "\n",
        "    # Exibir o resumo das emoções detectadas\n",
        "    print(\"\\nResumo das emoções detectadas:\")\n",
        "    for emotion, count in emotion_counts.items():\n",
        "        print(f\"{emotion}: {count} ocorrências\")\n",
        "\n",
        "    # Exibir o número de anomalias detectadas (categorias 'Unknown')\n",
        "    anomalies = emotion_counts.get(\"Unknown\", 0)\n",
        "    print(f\"\\nNúmero de anomalias detectadas: {anomalies}\")\n",
        "\n",
        "    # Criar o arquivo de relatório\n",
        "    report_path = os.path.join(os.path.dirname(output_path), 'relatorio_analise.txt')\n",
        "\n",
        "    with open(report_path, 'w') as report_file:\n",
        "        # Escrever o total de frames analisados\n",
        "        report_file.write(f\"Total de frames analisados: {total_frames}\\n\")\n",
        "\n",
        "        # Escrever o resumo das emoções detectadas\n",
        "        report_file.write(\"\\nResumo das emoções detectadas:\\n\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            report_file.write(f\"{emotion}: {count} ocorrências\\n\")\n",
        "\n",
        "        # Escrever o número de anomalias detectadas\n",
        "        report_file.write(f\"\\nNúmero de anomalias detectadas: {anomalies}\\n\")\n",
        "\n",
        "    print(f\"\\nRelatório salvo em: {report_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A847rr7sevtz",
        "outputId": "c55d19bd-cc32-4c5a-9456-d209a95c4007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pasta criada: /content/detected_faces\n"
          ]
        }
      ],
      "source": [
        "def criar_pasta_para_rostos(diretorio=\"detected_faces\"):\n",
        "    # Obter o caminho absoluto\n",
        "    caminho_absoluto = os.path.abspath(diretorio)\n",
        "\n",
        "    # Criar a pasta se ela não existir\n",
        "    if not os.path.exists(caminho_absoluto):\n",
        "        os.makedirs(caminho_absoluto)\n",
        "        print(f\"Pasta criada: {caminho_absoluto}\")\n",
        "    else:\n",
        "        print(f\"Pasta já existente: {caminho_absoluto}\")\n",
        "        return caminho_absoluto\n",
        "\n",
        "# Exemplo de uso\n",
        "pasta_rostos = criar_pasta_para_rostos()\n",
        "\n",
        "def ajustar_limites_rosto(box, width, height):\n",
        "\n",
        "    x1, y1, x2, y2 = box\n",
        "    x1, y1 = max(0, x1), max(0, y1)\n",
        "    x2, y2 = min(width, x2), min(height, y2)\n",
        "    return x1, y1, x2, y2\n",
        "def rastrear_ou_criar_id(box, face_trackers, proximo_id):\n",
        "\n",
        "    for existing_id, tracker_info in face_trackers.items():\n",
        "        tracked_box, _ = tracker_info\n",
        "        iou = sobreposicao(box, tracked_box)\n",
        "        if iou > 0.5:\n",
        "            return existing_id, proximo_id, face_trackers\n",
        "\n",
        "    face_id = proximo_id\n",
        "    proximo_id += 1\n",
        "    return face_id, proximo_id, face_trackers\n",
        "#face_roi, face_id, Rastreados, diretorio_saida\n",
        "\n",
        "def salvar_rosto(face_roi, face_id, Rastreados, diretorio_saida):\n",
        "\n",
        "    if face_id not in Rastreados:\n",
        "        Rastreados.add(face_id)\n",
        "        face_image_path = os.path.join(diretorio_saida, f\"face_{face_id}.jpg\")\n",
        "        cv2.imwrite(face_image_path, cv2.cvtColor(face_roi, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def sobreposicao(boxA, boxB):\n",
        "    # Calcular a interseção sobre união (IOU) entre dois retângulos\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def salvar_excel(emotions_per_id, output_path):\n",
        "    # Converter o dicionário em um DataFrame\n",
        "    data = [{\"ID\": face_id, \"Emoção Predominante\": emotion} for face_id, emotion in emotions_per_id.items()]\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Caminho para o arquivo Excel\n",
        "    excel_path = os.path.join(os.path.dirname(output_path), 'analise_emocoes.xlsx')\n",
        "\n",
        "    # Salvar o DataFrame no Excel\n",
        "    df.to_excel(excel_path, index=False)\n",
        "    print(f\"Arquivo Excel salvo em: {excel_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_hand_writing_or_holding(hand_landmarks):\n",
        "    \"\"\"\n",
        "    Detecta se uma mão está escrevendo ou manipulando algo baseado na proximidade dos dedos.\n",
        "    \"\"\"\n",
        "    # Coordenadas dos landmarks relevantes\n",
        "    thumb_tip = hand_landmarks[4]   # Ponta do polegar\n",
        "    index_tip = hand_landmarks[8]  # Ponta do dedo indicador\n",
        "    middle_tip = hand_landmarks[12]  # Ponta do dedo médio\n",
        "    ring_tip = hand_landmarks[16]  # Ponta do dedo anelar\n",
        "    pinky_tip = hand_landmarks[20]  # Ponta do dedo mínimo\n",
        "\n",
        "    # Distâncias entre polegar e os outros dedos\n",
        "    dist_thumb_index = ((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)**0.5\n",
        "    dist_thumb_middle = ((thumb_tip.x - middle_tip.x)**2 + (thumb_tip.y - middle_tip.y)**2)**0.5\n",
        "    dist_thumb_ring = ((thumb_tip.x - ring_tip.x)**2 + (thumb_tip.y - ring_tip.y)**2)**0.5\n",
        "    dist_thumb_pinky = ((thumb_tip.x - pinky_tip.x)**2 + (thumb_tip.y - pinky_tip.y)**2)**0.5\n",
        "\n",
        "    # Critério: todos os dedos próximos ao polegar indicam manipulação\n",
        "    threshold = 0.05  # Ajustar conforme o tamanho normalizado\n",
        "    is_holding = (\n",
        "        dist_thumb_index < threshold and\n",
        "        dist_thumb_middle < threshold and\n",
        "        dist_thumb_ring < threshold and\n",
        "        dist_thumb_pinky < threshold\n",
        "    )\n",
        "\n",
        "    return is_holding\n"
      ],
      "metadata": {
        "id": "-UL3cyKtKlVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m9c0_LeNexa0"
      },
      "outputs": [],
      "source": [
        "def detect_emotions(video_path, output_path):\n",
        "    # Capturar vídeo do arquivo especificado\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Verificar se o vídeo foi aberto corretamente\n",
        "    if not cap.isOpened():\n",
        "        print(\"Erro ao abrir o vídeo.\")\n",
        "        return\n",
        "\n",
        "    # Obter propriedades do vídeo\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Definir o codec e criar o objeto VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para MP4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Inicializar dicionário para contabilizar emoções\n",
        "    emotion_counts = {}\n",
        "    emotions_per_id = {}\n",
        "    pose_data = []\n",
        "    anomalies_detected = 0\n",
        "    total_frames_analyzed = 0\n",
        "    frames_sem_rosto = 0\n",
        "    frames_so_maos = 0\n",
        "    frames_maos_manipulando = 0\n",
        "\n",
        "    #prev_gray = None  # Frame anterior em escala de cinza\n",
        "\n",
        "    app = FaceAnalysis()\n",
        "    app.prepare(ctx_id=0)  # Use ctx_id=-1 for CPU\n",
        "\n",
        "    diretorio_saida=\"detected_faces\"\n",
        "\n",
        "    criar_pasta_para_rostos(diretorio=diretorio_saida)\n",
        "\n",
        "    Rastreador = {}\n",
        "    proximo_id = 1\n",
        "    Rastreados = set()\n",
        "\n",
        "    isolation_forest = IsolationForest(contamination=0.1)\n",
        "\n",
        "    maos = Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "    maos_sobre_a_face = 0\n",
        "    # Loop para processar cada frame do vídeo\n",
        "    for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
        "        # Ler um frame do vídeo\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Se não conseguiu ler o frame (final do vídeo), sair do loop\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "        faces = app.get(rgb_frame)\n",
        "        hands_results = maos.process(rgb_frame)\n",
        "\n",
        "        results = pose.process(rgb_frame)\n",
        "\n",
        "\n",
        "        #trecho luizhg\n",
        "        # Para landmarks de pose:\n",
        "        if results.pose_landmarks:\n",
        "          mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if len(faces) == 0:\n",
        "            frames_sem_rosto += 1\n",
        "\n",
        "            # Verificar se há mãos detectadas\n",
        "            if hands_results.multi_hand_landmarks:\n",
        "                frames_so_maos += 1\n",
        "                cv2.putText(frame, \"Hands only detected\", (10, height - 50),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
        "\n",
        "                if is_hand_writing_or_holding(hands_results):\n",
        "                  frames_maos_manipulando += 1\n",
        "                  cv2.putText(frame, \"Hands holding detected\", (10, height - 50),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
        "\n",
        "            elif results.pose_landmarks:\n",
        "                # Verificar landmarks dos braços (cotovelos e pulsos)\n",
        "                landmarks = results.pose_landmarks.landmark\n",
        "                left_elbow = landmarks[13]  # Cotovelo esquerdo\n",
        "                right_elbow = landmarks[14]  # Cotovelo direito\n",
        "                left_wrist = landmarks[15]  # Pulso esquerdo\n",
        "                right_wrist = landmarks[16]  # Pulso direito\n",
        "\n",
        "                # Calcular posições absolutas no frame\n",
        "                arm_landmarks = [\n",
        "                    (left_elbow.x, left_elbow.y),\n",
        "                    (right_elbow.x, right_elbow.y),\n",
        "                    (left_wrist.x, left_wrist.y),\n",
        "                    (right_wrist.x, right_wrist.y),\n",
        "                ]\n",
        "\n",
        "                arm_positions = [\n",
        "                    (int(lm[0] * width), int(lm[1] * height)) for lm in arm_landmarks\n",
        "                ]\n",
        "\n",
        "                # Marcar braços nos frames\n",
        "                for pos in arm_positions:\n",
        "                    cv2.circle(frame, pos, 5, (0, 255, 0), -1)\n",
        "\n",
        "                cv2.putText(frame, \"Arms only detected\", (10, height - 80),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        # Exibir contagem de frames no vídeo\n",
        "        #cv2.putText(frame, f\"No Faces: {frames_sem_rosto}, Hands Only: {frames_so_maos}\",(10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"No Faces: {frames_sem_rosto}, Hands Only: {frames_so_maos}, Writing: {frames_maos_manipulando}\", (10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Manter o controle do número de faces detectadas\n",
        "        if 'prev_num_faces' not in locals():\n",
        "            prev_num_faces = 0\n",
        "\n",
        "        num_faces = len(faces)\n",
        "\n",
        "        faces_text = f\"Number of faces changed from {prev_num_faces} to {num_faces}\"\n",
        "        cv2.putText(frame, faces_text, (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "        if num_faces != prev_num_faces:\n",
        "            prev_num_faces = num_faces\n",
        "\n",
        "        for face in faces:\n",
        "            box = face.bbox.astype(int)\n",
        "            x1, y1, x2, y2=ajustar_limites_rosto(box, width, height)\n",
        "\n",
        "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
        "\n",
        "            face_roi = rgb_frame[y1:y2, x1:x2]\n",
        "\n",
        "            ja_existe = False\n",
        "            id_face = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if face_roi.size == 0:\n",
        "                print(f\"Rosto inválido detectado no frame {_}, ignorando.\")\n",
        "                continue\n",
        "            else:\n",
        "                face_id, proximo_id, Rastreador = rastrear_ou_criar_id(box, Rastreador, proximo_id)\n",
        "                Rastreador[face_id] = ((x1, y1, x2, y2), _)\n",
        "                salvar_rosto(face_roi, face_id, Rastreados, diretorio_saida)\n",
        "\n",
        "            try:\n",
        "                # Chamar a função para detectar anomalias\n",
        "                #anomalies_detected = detectar_anomalia_mao(\n",
        "                #    prev_gray, face_roi, x1, y1, frame, anomalies_detected\n",
        "                #)\n",
        "\n",
        "                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "                if isinstance(analysis, list) and len(analysis) > 0:\n",
        "                    dominant_emotion = analysis[0]['dominant_emotion']\n",
        "                else:\n",
        "                    dominant_emotion = \"Unknown\"\n",
        "\n",
        "            except Exception as e:\n",
        "                dominant_emotion = \"Error\"\n",
        "                print(f\"Error analyzing emotion: {e}\")\n",
        "\n",
        "            # Atualizar contador de emoções\n",
        "            if dominant_emotion not in emotion_counts:\n",
        "                emotion_counts[dominant_emotion] = 0\n",
        "            emotion_counts[dominant_emotion] += 1\n",
        "\n",
        "            emotions_per_id[face_id] = dominant_emotion\n",
        "\n",
        "            cv2.putText(frame, f\"ID:{face_id} - {dominant_emotion}\",(box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,0.8, (0, 255, 255), 2)\n",
        "\n",
        "            if hands_results.multi_hand_landmarks:\n",
        "                      for hand_landmarks in hands_results.multi_hand_landmarks:\n",
        "                          hand_points = [(int(landmark.x * width), int(landmark.y * height))\n",
        "                                        for landmark in hand_landmarks.landmark]\n",
        "\n",
        "                          for point in hand_points:\n",
        "                              px, py = point\n",
        "                              if x1 <= px <= x2 and y1 <= py <= y2:\n",
        "                                  maos_sobre_a_face += 1\n",
        "                                  cv2.putText(frame, \"Hand over face detected\", (10, height - 50),\n",
        "                                              cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 165, 255), 2)\n",
        "                                  break\n",
        "\n",
        "    # Exibir contagem no frame\n",
        "            cv2.putText(frame, f\"Hands over face: {maos_sobre_a_face}\", (10, height - 70),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "\n",
        "\n",
        "\n",
        "            #prev_gray = gray_frame  # Atualizar o frame anterior\n",
        "\n",
        "        # Escrever o frame processado no arquivo de vídeo de saída\n",
        "        out.write(frame)\n",
        "\n",
        "        # Analisar dados de pose para detectar anomalias\n",
        "    if len(pose_data) > 10:\n",
        "        pose_data = np.array(pose_data)\n",
        "        anomaly_labels = isolation_forest.fit_predict(pose_data)\n",
        "        anomalies_detected = np.sum(anomaly_labels == -1)\n",
        "\n",
        "    with open(os.path.join(os.path.dirname(output_path), 'relatorio_analise.txt'), 'a') as report_file:\n",
        "        report_file.write(f\"\\nNúmero de anomalias detectadas (movimentos anômalos): {anomalies_detected}\\n\")\n",
        "\n",
        "    # Chamar a função para gerar o relatório\n",
        "    generate_summary(emotion_counts, total_frames, output_path)\n",
        "\n",
        "    salvar_excel(emotions_per_id, output_path)\n",
        "\n",
        "    # Liberar a captura de vídeo e fechar todas as janelas\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    #cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEOn56zZe59J",
        "outputId": "08b91074-4743-4137-be55-e7597df56e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n",
            "Pasta criada: /content/detected_faces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando vídeo: 100%|██████████| 90/90 [04:11<00:00,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RELATÓRIO:\n",
            "\n",
            "Total de frames analisados: 90\n",
            "\n",
            "Resumo das emoções detectadas:\n",
            "sad: 117 ocorrências\n",
            "neutral: 95 ocorrências\n",
            "angry: 75 ocorrências\n",
            "fear: 60 ocorrências\n",
            "happy: 12 ocorrências\n",
            "surprise: 1 ocorrências\n",
            "\n",
            "Número de anomalias detectadas: 0\n",
            "\n",
            "Relatório salvo em: /content/relatorio_analise.txt\n",
            "Arquivo Excel salvo em: /content/analise_emocoes.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "script_dir = os.getcwd()\n",
        "input_video_path = \"/content/video_cut-original-3seg.mp4\"\n",
        "output_video_path = \"/content/output_video_16.mp4\"  # Nome do vídeo de saída\n",
        "\n",
        "# Chamar a função para detectar emoções no vídeo e salvar o vídeo processado\n",
        "detect_emotions(input_video_path, output_video_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}